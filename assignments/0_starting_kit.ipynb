{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG2MLbQvZy9h"
      },
      "source": [
        "# **The Problem: Automatic Apparent Age Estimation**\n",
        "Author: Julio C. S. Jacques Junior\n",
        "\n",
        "Last modified: Feb 24, 2023.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Objectives**:\n",
        "\n",
        "- Play with different machine learning concepts incrementaly.\n",
        "\n",
        "- Define new strategies to **improve accuracy** performance (overall and per attribute) **while minimizing the different bias scores**.\n",
        "\n",
        "- Based on your experiments, be able to provide a strong analysis and discussion of the results when delivering your report.\n",
        "\n",
        "- **IMPORTANT:** as you will see, the problem is basicaly solved in this starting-kit. However, **we expect you to to beyond the starting-kit**. We expect you to play with different hyperparameters, training strategies, different backbones, etc. Avoid making minor changes on the provided code as your final solution. You won't be evaluated based on the accuracy your model provide, but based on your creativity and the way you define the experiments, report and discuss the results (using the report document template - more information will be provided on Virtual Campus).\n",
        "- **NOTE:** the preliminary discussion provided in this Colab file is based on the results obtained on the **validation set**. You should be able to train different models and evaluate different strategies using the training and validation set. When you are done (that is, when you are satisfied with your results), you can obtain **the final results on the TEST** set using the competition webpage we created for this course. That is, you should not have access to the test labels. We consider this a good excercise where we try to simulate a real situation where people don't have access to the test labels (also usefull to avoid overfitting on the test data). The details about the competition can be found on Virtual Campus.\n"
      ],
      "metadata": {
        "id": "hxuyVH2sGhFj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwmfFS5oPbhn"
      },
      "source": [
        "# Auxiliary and introductory material\n",
        "\n",
        "Getting Started with TensorFlow in Google Colaboratory\n",
        "Intro to Google Colab:\n",
        "https://www.youtube.com/watch?v=inN8seMm7UI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtnpcwuJon27"
      },
      "source": [
        "# Pre-requisites and important notes:\n",
        "- Installing tensorflow and OpenCv.\n",
        "- Check Colab GPU usage instructions [here](https://research.google.com/colaboratory/faq.html#gpu-availability)\n",
        "\n",
        "- This code was tested on tensorflow 2.8.2\n",
        "\n",
        "- Sometimes the code downloads data/models from our server. It may happen that you get a \"file not found\" error due to some instability of the server. In this case, please keep trying! If the error persist, please contact me."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnRxTFRGorml"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.8.2\n",
        "!pip install opencv-python\n",
        "!pip install h5py\n",
        "\n",
        "# to enable Colab-GPU version go to \n",
        "# \"Runtime\" -> \"Change runtime type\" and select GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm4gzuIxqwQQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMULLE13o-Qo"
      },
      "source": [
        "# Downloading and decompressing the [Appa-Real Age Dataset](http://chalearnlap.cvc.uab.es/challenge/13/track/13/description/)\n",
        "\n",
        "- Original RGB images (cropped faces) are in the range of [0, 255] and labels are in the range of ~0.9 to ~90 (years old). You will see later that we will re-scale some values to be in the range of [0,1] to make some operations (e.g., training) and eventualy re-scalling the output back (e.g., during evaluation) using a pre-defined normalization factor.\n",
        "- The data is divided in train, validation and test set. However, **we will omit the Test labels to make the practical sessions more close to a real scenario.** \n",
        "- You can train your models with the provided train set and validate it on the validation set, as illustrated below, or you can build your ouwn train/validation set by, for example, merging the provided train/validation data and randomly split them. You are free to define any training strategy. **Your creativity will have a considerable impact on evaluation.**\n",
        "- Matadata is provided\n",
        "  - gender: male / female \n",
        "  - ethnicity: asian / afroamerican / caucasian\n",
        "  - facial expression: neutral / slightlyhappy / happy / other\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8DZFR8GpIfL"
      },
      "outputs": [],
      "source": [
        "# downloading the data\n",
        "!wget http://data.chalearnlap.cvc.uab.cat/Colab_2021/app_data_with-no-test-labels.zip\n",
        "\n",
        "# decompressing the data\n",
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile('app_data_with-no-test-labels.zip','r') as zip:\n",
        "   zip.extractall()\n",
        "   print('Data decompressed successfully')\n",
        "\n",
        "# removing the .zip file after extraction to clean space\n",
        "!rm app_data_with-no-test-labels.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a18X0kFGXUnt"
      },
      "source": [
        "# Loading the train/validation data, and re-scaling the labels to [0,...,1]\n",
        "- X_[train,valid,test] = Face images\n",
        "- Y_[train,valid,test] = Ground truth \n",
        "- M_[train,valid,test] = Metadata (gender, ethnicicy, facial expression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GRhB1gzXlYc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# loading the train data (with labels)\n",
        "X_train = np.load('./data/data_train.npy')\n",
        "Y_train = np.load('./data/labels_train.npy')\n",
        "M_train = np.load('./data/meta_data_train.npy')\n",
        "\n",
        "# loading the validation data (with labels)\n",
        "X_valid = np.load('./data/data_valid.npy')\n",
        "Y_valid = np.load('./data/labels_valid.npy')\n",
        "M_valid = np.load('./data/meta_data_valid.npy')\n",
        "\n",
        "# loading the test data (images only) to be used later in our challenge\n",
        "X_test = np.load('./data/data_test.npy')\n",
        "\n",
        "\n",
        "# train labels are real numbers, ranging from ~0.9 to ~89 (years old);\n",
        "# we will re-scale the labels to [0,1] by using a normalization factor of 100,\n",
        "# assuming there is no sample with age > 100.\n",
        "Y_train = Y_train/100\n",
        "Y_valid = Y_valid/100\n",
        "\n",
        "print('Train data size and shape', X_train.shape)\n",
        "print('Train labels size and shape', Y_train.shape)\n",
        "print('Train metadata size and shape', M_train.shape)\n",
        "print('----')\n",
        "print('Valid data size and shape', X_valid.shape)\n",
        "print('Valid labels size and shape', Y_valid.shape)\n",
        "print('Valid metadata size and shape', M_valid.shape)\n",
        "print('----')\n",
        "print('Test data size and shape', X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HVnh9fvUvoJ"
      },
      "source": [
        "# Visualizing some training samples\n",
        "Next, we multiply the normalized age labels by 100 to show the original age values on top of each sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxnI7M8s-JXf"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(20, 20))\n",
        "for i, ax in enumerate(axes):\n",
        "    idx = random.randint(0, len(X_train))\n",
        "    ax.imshow(cv2.cvtColor(X_train[idx,:,:,:], cv2.COLOR_BGR2RGB))\n",
        "    ax.set_title(Y_train[idx]*100)\n",
        "    ax.set(xlabel=[M_train[idx][0],M_train[idx][1],M_train[idx][2]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDzps0ccD2pR"
      },
      "source": [
        "# Visualizing the age distribution of Train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMCqsJtBD8KG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
        "fig.suptitle('Age distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "# labels are multiplied by 100 to show the original values\n",
        "ax1.hist(Y_train*100, bins = 50)\n",
        "ax1.set_title('Y_train labels')\n",
        "ax1.set(xlabel='Apparent age', ylabel='Num. of samples')\n",
        "ax1.set_xlim([0, 100])\n",
        "\n",
        "ax2.hist(Y_valid*100, bins = 50)\n",
        "ax2.set_title('Y_valid labels')\n",
        "ax2.set(xlabel='Apparent age', ylabel='Num. of samples')\n",
        "ax2.set_xlim([0, 100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uShj-2u-9HCD"
      },
      "source": [
        "# Visualizing the distributions of metadata (Train data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SCU4nm5-qY2"
      },
      "outputs": [],
      "source": [
        "gender = []\n",
        "etnhicity = []\n",
        "emotion = []\n",
        "for sample in M_train:\n",
        "  gender.append(sample[0])\n",
        "  etnhicity.append(sample[1])\n",
        "  emotion.append(sample[2])\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 4))\n",
        "fig.suptitle('Metadata distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "ax1.hist(gender)\n",
        "ax2.hist(etnhicity)\n",
        "ax3.hist(emotion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOc3kPMYzTCI"
      },
      "source": [
        "# Preprocessing the data (face images)\n",
        "- Later, we will define our model based on ResNet50 (our backbone). Originally,\n",
        "ResNet50 uses a regularization that changes the range of the input images. Thus,\n",
        "to be aligned with the ResNet50 input, we preprocess our input images using the respective 'preprocess_input' function. Later, if you decide to use another model as backbone (rather than ResNet), you may skip the following preprocessing stage. Note, **this notebook is used as starting-kit and you are free to use any model architecture, pre-trained model and training strategy.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42bGw2eCzZ2r"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "for i in range(0,X_train.shape[0]):\n",
        "  x = X_train[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_train[i,] = tf.keras.applications.resnet50.preprocess_input(x)\n",
        "\n",
        "# validation\n",
        "for i in range(0,X_valid.shape[0]):\n",
        "  x = X_valid[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_valid[i,] = tf.keras.applications.resnet50.preprocess_input(x)  \n",
        "\n",
        "# test\n",
        "for i in range(0,X_test.shape[0]):\n",
        "  x = X_test[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_test[i,] = tf.keras.applications.resnet50.preprocess_input(x)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGjZNPq_r9I7"
      },
      "source": [
        "# Downloading the ResNet50 model pre-trained on Faces\n",
        "We are using ResNet50 pre-trained on Faces (source [here](https://github.com/ox-vgg/vgg_face2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQwDAOner6rD"
      },
      "outputs": [],
      "source": [
        "# downloading the data\n",
        "!wget http://data.chalearnlap.cvc.uab.cat/Colab_2021/model.zip\n",
        "\n",
        "# decompressing the data\n",
        "with ZipFile('model.zip','r') as zip:\n",
        "   zip.extractall()\n",
        "   print('Model decompressed successfully')\n",
        "\n",
        "# removing the .zip file after extraction  to clean space\n",
        "!rm model.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a29243uAmxUS"
      },
      "source": [
        "# Loading the pre-trained model\n",
        "- You can see the data we have downloaded and the structure of Colab by clicking on 'Files', on the left side (<--) of this interface.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J--yWIXom0uf"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "# loading the pretrained model\n",
        "model = tf.keras.models.load_model('./model/weights.h5')\n",
        "\n",
        "# print the model summary\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrDpADCzzfjU"
      },
      "source": [
        "# Adapting the model to our needs\n",
        "- In summary, we will ignore the last layer 'classifier_low_dim' and will include a few other layers on top of our backbone. Here, we also define the activation function we are going to use as output of the last FC layer (Sigmoid, in the case). Again, **this notebook is expected to be used as starting-kit. For the deliverabels, avoid simply making minor changes to it. Instead, exploit your creativity as much as you can.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ABLExjBziRn"
      },
      "outputs": [],
      "source": [
        "# Using the FC layer before the 'classifier_low_dim' layer as feature vector\n",
        "fc_512 = model.get_layer('dim_proj').output\n",
        "\n",
        "# adding a dropout layer to minimize overfiting problems\n",
        "dp_layer = tf.keras.layers.Dropout(0.5)(fc_512)\n",
        "\n",
        "# adding a few hidden FC layers to learn hidden representations\n",
        "fc_128 = tf.keras.layers.Dense(128, activation='relu', name='f_128')(fc_512)\n",
        "fc_32 = tf.keras.layers.Dense(32, activation='relu', name='f_32')(fc_128)\n",
        "\n",
        "# Includint an additional FC layer with sigmoid activation, used to regress\n",
        "# the apparent age\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid', name='predict')(fc_32)\n",
        "\n",
        "# building and pringing the final model\n",
        "model = tf.keras.models.Model(inputs=model.get_layer('base_input').output,outputs=output)\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq_-Zh3Bt5Gx"
      },
      "source": [
        "# Freezing the first layers to allow the fine-tuning of the last FC layers (only)\n",
        "- Next, we set some layer to be trainable or not, and print if layers are set to trainable = True or False.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pfL6NhouTe-"
      },
      "outputs": [],
      "source": [
        "counter = 0\n",
        "for layer in model.layers:\n",
        "  if counter <= 174: \n",
        "    layer.trainable = False\n",
        "  else:\n",
        "    layer.trainable = True\n",
        "  print(counter, layer.name, layer.trainable)\n",
        "  counter +=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM5L7P0EvVYn"
      },
      "source": [
        "# Printing the MODEL (summary) we have just defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqipjTHzM6np"
      },
      "outputs": [],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwG95hLURSGj"
      },
      "source": [
        "# IMPORTANT: Mounting your google drive to save your results\n",
        "- Colab gives you LIMITED GPU access. Thus, it may kill your process (of training) if you pass a limited amount of training hours. To allow you to save your model while training, you can mount your google drive, as detailed next. This way, if the process is killed, you can (in a new session) load your checkpoints (trained model, from your google drive) and, for example, continue training or make predictions with the model you obtained (even if trained for a few epochs).\n",
        "- In the following examples, the **the beset model (based on validation loss) is saved in my google drive inside a \"/temp/\" directory. You will need to addapt this path to your case.** \n",
        "- To save time, and to allow you to quickly 'play' and run the notebook, we have pre-trained some models, which are loaded (or not) based on some boolean variables (later, you will need to change/adapt these codes to achive the goals of this course)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7sByyqKRae5"
      },
      "outputs": [],
      "source": [
        "#--------------------------\n",
        "MOUNT_GOOGLE_DRIVE = False\n",
        "#--------------------------\n",
        "\n",
        "if(MOUNT_GOOGLE_DRIVE==True):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  # Note, the default path will be: '/content/gdrive/MyDrive/'\n",
        "  # In my case, the final path will be: '/content/gdrive/MyDrive/temp/' as I\n",
        "  # created a '/temp/' folder in my google drive for this purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n65k0Gc4vXRS"
      },
      "source": [
        "# Training the Model / or downloading a model already trained\n",
        "- As default, the code below will load a pre-trained model, obtained using the same code if LOAD_BEST_MODEL_ST1 is set to False.\n",
        "- Later, you can set LOAD_BEST_MODEL_ST1 to False to perfom the training.\n",
        "  - The code below uses Early stopping (es) with patience = 5 (that is, the training will stop if no improvement on valid_loss is observed on the last 5 epochs).\n",
        "  - It uses the Mean Squared Error (MSE) as loss function ('loss=tf.keras.losses.MeanSquaredError()'). The code also evaluates the Mean Absolute Error (MAE) during training ('metrics=['mae']'). Learning rate is set to 'learning_rate=1e-5', batch size = 32, and the model will be trained for 50 epochs (if Colab allows it based to the time budget)\n",
        "  - The model callback (mc) is set to save the best model based on valid_loss (that is, if validation loss decreases from one epoch to another, a new model is saved on the path you specify).\n",
        "  - Other hyperparameters you can play with are: defining another optimizer, loss function, learning rate, batch size, num of epochs.\n",
        "\n",
        "- Note: in case you want to save your model, stop training, and resume training, check the end of this file **\"II) illustrating how to train + save + stop training + RESUME TRAINING\"** where we provide a more detailed example about this procedure. Recommendation: first train your model for a few epochs to avoid the need of resume training. This way, you will get used with the code and the general pipeline. Later, you can play with that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71Oh2HLcz2dg"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# load a model and train history (defined and trained\n",
        "# as below, trained for 38 epochs)\n",
        "#--------------------------\n",
        "LOAD_BEST_MODEL_ST1 = True # (training only the last FC layers)\n",
        "#--------------------------\n",
        "\n",
        "\n",
        "if(LOAD_BEST_MODEL_ST1==True):\n",
        "  # downloading the trained model\n",
        "  !wget http://data.chalearnlap.cvc.uab.cat/Colab_2021/best_model_st1.zip\n",
        "  # decompressing the data\n",
        "  with ZipFile('best_model_st1.zip','r') as zip:\n",
        "    zip.extractall()\n",
        "    print('Model decompressed successfully')\n",
        "  # removing the .zip file after extraction  to clean space\n",
        "  !rm best_model_st1.zip\n",
        "  \n",
        "else:\n",
        "  # defining the early stop criteria\n",
        "  es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "  # saving the best model based on val_loss\n",
        "  mc = tf.keras.callbacks.ModelCheckpoint('/content/gdrive/MyDrive/temp/best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "  # defining the optimizer\n",
        "  model.compile(tf.keras.optimizers.Adam(learning_rate=1e-4),loss=tf.keras.losses.MeanSquaredError(),metrics=['mae'])\n",
        "\n",
        "  # training the model\n",
        "  history = model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), batch_size=32, epochs=50, shuffle=True, verbose=1, callbacks=[es,mc])\n",
        "  \n",
        "  # saving training history (for future visualization)\n",
        "  with open('/content/gdrive/MyDrive/temp/train_history.pkl', 'wb') as handle:\n",
        "    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U1hPksD9BYt"
      },
      "source": [
        "# Visualizing the train history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRKbcPYNOFRx"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# here, it loads the history of the model we have already trained, or loads the \n",
        "# history of the model you defined and trained\n",
        "if(LOAD_BEST_MODEL_ST1==True):\n",
        "  train_hist = pickle.load(open(\"train_history.pkl\",\"rb\"))\n",
        "else:\n",
        "  train_hist = pickle.load(open(\"/content/gdrive/MyDrive/temp/train_history.pkl\",\"rb\"))\n",
        "\n",
        "# we plot both, the LOSS and MAE\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
        "fig.suptitle('Training history (stage 1)', fontsize=14, fontweight='bold')\n",
        "\n",
        "ax1.plot(train_hist['loss'])\n",
        "ax1.plot(train_hist['val_loss'])\n",
        "ax1.set(xlabel='epoch', ylabel='LOSS')\n",
        "ax1.legend(['train', 'valid'], loc='upper right')\n",
        "\n",
        "ax2.plot(train_hist['mae'])\n",
        "ax2.plot(train_hist['val_mae'])\n",
        "ax2.set(xlabel='epoch', ylabel='MAE')\n",
        "ax2.legend(['train', 'valid'], loc='upper right')  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDGOQjFz4k90"
      },
      "source": [
        "# Loading the saved model and making predictions on the Validation set\n",
        "- Next, we load the trained model and make predictions on the Validation set (for debug purpose, if needed).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4OKbe-3GBMB"
      },
      "outputs": [],
      "source": [
        "#--------------------------\n",
        "ENABLE_EVALUATION_ST1 = True\n",
        "#--------------------------\n",
        "\n",
        "# loading the saved model\n",
        "if(LOAD_BEST_MODEL_ST1==True):\n",
        "  saved_model = tf.keras.models.load_model('best_model.h5')\n",
        "else:\n",
        "  saved_model = tf.keras.models.load_model('/content/gdrive/MyDrive/temp/best_model.h5')\n",
        "\n",
        "if(ENABLE_EVALUATION_ST1==True):\n",
        "  # predict on the validation data (normalized outputs)\n",
        "  predictions_st1_valid = saved_model.predict(X_valid, batch_size=32, verbose=1)\n",
        "\n",
        "  # re-scaling the predictions to the range of \"age\" as the outputs are in the range of [0,1]\n",
        "  predictions_st1_valid_final = predictions_st1_valid*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3cP34WQI2pc"
      },
      "source": [
        "# Re-evaluating the model on the Validation set using the Mean Absolute Error (MAE) as metric (for debug purpose) and printing some predicted results.\n",
        "- Note, as the train/validation labels were re-scaled to be in the range of [0,1] for training, the predictions will be in the same range [0,1]. Thus, we re-scale them back (to be in the range of \"ages\") in different sections of this notebook to facilitate the analysis, e.g., when printing output predictions or during evaluations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqdlJy5HGppZ"
      },
      "outputs": [],
      "source": [
        "if(ENABLE_EVALUATION_ST1==True):\n",
        "  # evaluating on validation data (re-scalling Y_labels back to the range of \"ages\", using the normalization factor)\n",
        "  error = []\n",
        "  for i in range(0,len(Y_valid)):\n",
        "    error.append(abs(np.subtract(predictions_st1_valid_final[i][0],Y_valid[i]*100)))\n",
        "\n",
        "  print('MAE (validation) = %.8f' %(np.mean(error)))\n",
        "\n",
        "  # printing some predictions (re-scaling back the values to the \"age\" range, using the normalization factor defined earlier)\n",
        "  print('-----')\n",
        "  for i in range(0,10):\n",
        "    print('predicted age = %.3f - Ground truth = %.3f' %(predictions_st1_valid_final[i], Y_valid[i]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VITEsCYfHi6q"
      },
      "source": [
        "---\n",
        "---\n",
        "# Performing a 2nd Stage of training, given the previously trained model, where all layers are set to \"trainable\"\n",
        "- Up to here, we have just trained the last FC layers of our model. Now, we will load the model we have trained (referred to it as 1st stage), set all layers to TRAINABLE, and train the whole model. Training will take more time, but we expect to get better results.\n",
        "- **Note that you are totally free to defien your own training strategy (e.g., using one single stage or multiple stages, freezing as many layers or blocks you want). Please, avoid simply reusing the starting-kit with minor changes, as your creativity will be considered during evaluation.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0vM0ZCzH6_L"
      },
      "outputs": [],
      "source": [
        "# setting all layers of the model to trainable\n",
        "saved_model.trainable = True\n",
        "\n",
        "counter = 0\n",
        "for layer in saved_model.layers:\n",
        "  print(counter, layer.name, layer.trainable)\n",
        "  counter +=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRrI2Vn9P_bw"
      },
      "source": [
        "# Training the whole model (2nd Stage of training)\n",
        "\n",
        "- As default, the code below will load a pre-trained model, obtained using the same code if LOAD_BEST_MODEL_ST2 is set to False.\n",
        "- Later, you can set LOAD_BEST_MODEL_ST2 to False to perfom the training.\n",
        "  - As before, the code below uses Early stopping (es) with patience = 5 (that is, the training will stop if no improvement on valid_loss is observed on the last 5 epochs).\n",
        "  - It uses the Mean Squared Error (MSE) as loss function ('loss=tf.keras.losses.MeanSquaredError()'). The code also evaluates the Mean Absolute Error (MAE) during training ('metrics=['mae']'). Learning rate is set to 'learning_rate=1e-5', batch size = 16, and the model will be trained for 12 epochs (if Colab allows it based to the time budget). Note, if you increase the batch size too much, data may not fit the GPU capacity (as the number of parameters to train increased compared to the 1st stage). This is why we reduced it from 32 to 16.\n",
        "  - The model callback (mc) is set to save the best model based on valid_loss (that is, if validation loss decreases from one epoch to another, a new model is saved on the path you specify).\n",
        "  - Other hyperparameters you can play with are: defining another optimizer, loss function, learning rate, batch size, num of epochs.\n",
        "- WARNING: at this stage, training take more time, and colab may close before you finish training due to time constraints. Thus, you will need to define a good strategy! In case you want to save your model, stop training, and resume training, check the end of this file **\"II) illustrating how to train + save + stop training + RESUME TRAINING\"** where we provide a more detailed example about this procedure.\n",
        "- WARNING: if you save your model and resume training, the train history will be lost. To monitore the training history, you may need to save the train history in another way (e.g., you can copy and paste the logs into a text file before resuming the training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq9lGU2p1b47"
      },
      "outputs": [],
      "source": [
        "# Loding a pretrained model or train\n",
        "#--------------------------\n",
        "LOAD_BEST_MODEL_ST2 = True\n",
        "#--------------------------\n",
        "\n",
        "if(LOAD_BEST_MODEL_ST2==True):\n",
        "  # downloading the trained model\n",
        "  !wget http://data.chalearnlap.cvc.uab.cat/Colab_2021/best_model_st2.zip\n",
        "  # decompressing the data\n",
        "  with ZipFile('best_model_st2.zip','r') as zip:\n",
        "    zip.extractall()\n",
        "    print('Model decompressed successfully')\n",
        "  # removing the .zip file after extraction  to clean space\n",
        "  !rm best_model_st2.zip\n",
        "\n",
        "else:\n",
        "  \n",
        "  # training all layers (2nd stage), given the model saved on stage 1\n",
        "  saved_model.compile(tf.keras.optimizers.Adam(learning_rate=1e-5),loss=tf.keras.losses.MeanSquaredError(),metrics=['mae'])\n",
        "\n",
        "  # defining the early stop criteria\n",
        "  es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "  # saving the best model (2nd stage) based on val_loss with a different name\n",
        "  mc = tf.keras.callbacks.ModelCheckpoint('/content/gdrive/MyDrive/temp/best_model_2nd_stage.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "  history = saved_model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), batch_size=16, epochs=12, shuffle=True, verbose=1, callbacks=[es,mc])\n",
        "\n",
        "  # saving training history\n",
        "  with open('/content/gdrive/MyDrive/temp/train_history_2nd_stage.pkl', 'wb') as handle:\n",
        "    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5hRW7Ryl39L"
      },
      "source": [
        "# Visualizing the Training history of both stages (1st stage and 2nd stage)\n",
        "- Next, we show the train history of the models we have already trained for this course.\n",
        "- The curves of both stages are concatenated. You can observe a peak close to epoch 42, which indicates the end of the first stage of training and the start of the 2nd stage, and how the 2nd stage improves the performance with respect to validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv-M1AhXuJJz"
      },
      "outputs": [],
      "source": [
        "if(LOAD_BEST_MODEL_ST1==True and LOAD_BEST_MODEL_ST2==True):\n",
        "  train_hist = pickle.load(open(\"train_history.pkl\",\"rb\"))\n",
        "  train_hist_2nd = pickle.load(open(\"train_history_2nd_stage.pkl\",\"rb\"))\n",
        "else:\n",
        "  train_hist = pickle.load(open(\"/content/gdrive/MyDrive/temp/train_history.pkl\",\"rb\"))\n",
        "  train_hist_2nd = pickle.load(open(\"/content/gdrive/MyDrive/temp/train_history_2nd_stage.pkl\",\"rb\"))\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
        "fig.suptitle('Training history (Stage 1 and Stage 2)', fontsize=14, fontweight='bold')\n",
        "\n",
        "ax1.plot(train_hist['loss']+train_hist_2nd['loss'])\n",
        "ax1.plot(train_hist['val_loss']+train_hist_2nd['val_loss'])\n",
        "ax1.set(xlabel='epoch', ylabel='Loss')\n",
        "ax1.legend(['train', 'valid'], loc='upper right')\n",
        "\n",
        "ax2.plot(train_hist['mae']+train_hist_2nd['mae'])\n",
        "ax2.plot(train_hist['val_mae']+train_hist_2nd['val_mae'])\n",
        "ax2.set(xlabel='epoch', ylabel='MAE')\n",
        "ax2.legend(['train', 'valid'], loc='upper right')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6wDuxn8mJ8y"
      },
      "source": [
        "# Re-evaluating the model lerned at the 2nd stage on the Validation set and printing some results\n",
        "- Next, you can observe a decrease in Mean Absolute Error (MAE) compared to the predictions obtained with the model trained at the 1st stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGyp-rzFmRhA"
      },
      "outputs": [],
      "source": [
        "#--------------------------\n",
        "ENABLE_EVALUATION_ST2 = True\n",
        "#--------------------------\n",
        "\n",
        "# loading the saved model\n",
        "if(LOAD_BEST_MODEL_ST2 == True):\n",
        "  saved_model_2nd = tf.keras.models.load_model('best_model_2nd_stage.h5')\n",
        "else:\n",
        "  saved_model_2nd = tf.keras.models.load_model('/content/gdrive/MyDrive/temp/best_model_2nd_stage.h5')\n",
        "\n",
        "\n",
        "if(ENABLE_EVALUATION_ST2==True):\n",
        "  # predict on the validation data\n",
        "  predictions_st2_valid = saved_model_2nd.predict(X_valid, batch_size=32, verbose=1)\n",
        "\n",
        "  # re-scaling the predictions to the range of \"age\" as the outputs are in the range of [0,1]\n",
        "  predictions_st2_valid_final = predictions_st2_valid*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSAb3Fl00Gwu"
      },
      "outputs": [],
      "source": [
        "if(ENABLE_EVALUATION_ST2==True):\n",
        "  # evaluating on test data (and re-scalling the labels back to the range of \"ages\")\n",
        "  error = []\n",
        "  for i in range(0,len(Y_valid)):\n",
        "    error.append(abs(np.subtract(predictions_st2_valid_final[i][0],Y_valid[i]*100)))\n",
        "\n",
        "  print('MAE (validation) = %.8f' %(np.mean(error)))\n",
        "\n",
        "  # printing some predictions (in the range of \"ages\", \n",
        "  # after re-scaling the ground truth values back, using the normalization factor)\n",
        "  print('-----')\n",
        "  for i in range(0,10):\n",
        "    print('predicted age = %.3f - Ground truth = %.3f' %(predictions_st2_valid_final[i], Y_valid[i]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR_nPmpJ_Gln"
      },
      "source": [
        "---\n",
        "---\n",
        "# Making predictions on the **Test set** and saving the results (predictions.zip) to be uploaded later to our Codalab Competition\n",
        "- Next, we show how you can save your predictions (on the test set) so that they can be submitted to our Codalab Competition.\n",
        "- Note, the .csv file MUST be named **'predictions.csv'**, othwerwise you will get an error on Codalab.\n",
        "- The generated zip file can be submitted on codalab ( <- you can download in from the left part of this web interface).\n",
        "- You can reuse and adapt this code to generate a submission file for the different results you will obtain during the course.\n",
        "- Note that in this example we will use the model learned above, resulted from the 2nd stage of training.\n",
        "\n",
        "> To enter the competition you need to create an account on [Codalab](https://codalab.lisn.upsaclay.fr), register to our competition and submit your predictions to our challenge: **Challenge link and additional instructions are centralized in eCampus**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN2q5kteKIwz"
      },
      "outputs": [],
      "source": [
        "# making predictions on the TEST data\n",
        "predictions_st2_test = saved_model_2nd.predict(X_test, batch_size=32, verbose=1)\n",
        "\n",
        "# re-scaling the predictions to the range of \"ages\" as the outputs are in the range of [0,1]\n",
        "predictions_st2_test_final = predictions_st2_test*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1zWzVF2DwHl"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# saving the predictions as a csv file\n",
        "with open('predictions.csv', 'w') as csvFile:\n",
        "  writer = csv.writer(csvFile)\n",
        "  writer.writerows(predictions_st2_test_final)\n",
        "csvFile.close()\n",
        "\n",
        "# compressing the csv file (to be submitted to codalab as prediction)\n",
        "! zip predictions.zip predictions.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXQZXojAxGJc"
      },
      "source": [
        "---\n",
        "---\n",
        "############### **Bias mitigation strategies** ###############\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# Accuracy is not enough! We also need to evaluate how biased is our model!\n",
        "- Next, we define different different functions, used to compute a bias score given different attributes.\n",
        "  - Age bias (4 sub-groups)\n",
        "  - Gender bias (2 sub-groups) \n",
        "  - Ethnicity bias (3 sub-groups)\n",
        "  - Facial Expression bias (4 sub-groups)\n",
        "- In a nutshell, given a particular attribute $A$, we compute the Mean Absolute Error $E_n$ for its $N$ sub-groups. We illustrate this process for the case of **Age** next:\n",
        "- Consider we have 4 sub-groups base on different age ranges (i.e., \"0-19\", \"20-39\", \"40-59\", and \"60-100\"). We will have one error value per sub-group: $E_1$, $E_2$, $E_3$ and $E_4$.\n",
        "- Then, we compute the absolute difference among them all. Consider $D$ is a squared matrix where each element $(i,j)$ is the absolute difference between $E_i$ and $E_j$. Then we can retrieve:\n",
        "  - $D_{2,1} = |E_1-E_2|$\n",
        "  - $D_{3,1} = |E_1-E_3|$\n",
        "  - $D_{4,1} = |E_1-E_4|$\n",
        "  - $D_{3,2} = |E_2-E_3|$\n",
        "  - $D_{4,2} = |E_2-E_4|$\n",
        "  - $D_{4,3} = |E_3-E_4|$\n",
        "\n",
        "- The Final Bias score $B_A$ for attribute $A$ is obtained by the Average of the computed differences. That is:\n",
        "  - $B_A = \\frac{(D_{2,1} + D_{3,1} + D_{4,1} + D_{3,2} + D_{4,2} + D_{4,3})}{6}$\n",
        "  - In other words: \n",
        "\n",
        "   $B_A = \\frac{1}{(N^2-N)/2}\\sum_{i=1}^{N} \\sum_{j=1}^{N} |E_i - E_j|, \\forall i,j \\in \\mathbb{N}^*, \\text{if } i < j$\n",
        "\n",
        "- To minimize your bias score, given a particular attribute, you will need to minimize the absolute difference among the different sub-groups being evaluated. That is, part of your goal will be to make the $N$ sub-groups of each attribute $A$ to have similar errors $E_n$\n",
        "- The big challenge here is to minimize **ALL** bias scores (i.e., age, gender, ethnicity and face expression).\n",
        "- Next, we briefly detail the different attributes (and their sub-groups) where the bias scores are evaluated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlPY4wbeUr_7"
      },
      "source": [
        "---\n",
        "# Age Bias ($B_a$) \n",
        "\n",
        "- Evaluates how accurate the model is with respect to different age ranges.\n",
        "  - sub-group 1: age < 20\n",
        "  - sub-group 2: 20 <= age < 40\n",
        "  - sub-group 3: 40 <= age < 60\n",
        "  - sub-group 4: 60 <= age\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALWjE5J8edq7"
      },
      "source": [
        "# Gender Bias ($B_g$) \n",
        "- Evaluates how accurate the model is with respect to different gender.\n",
        "  - sub-group 1: male\n",
        "  - sub-group 2: female\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8rIPUIrjkDG"
      },
      "source": [
        "# Ethnicity Bias ($B_e$)\n",
        "- Evaluates how accurate the model is with respect to different ethnicity categories.\n",
        "  - sub-group 1: asian\n",
        "  - sub-group 2: afroamerican\n",
        "  - sub-group 3: caucasian\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohPkAMvxm9Ed"
      },
      "source": [
        "# Face expression bias ($B_f$)\n",
        "- Evaluates how accurate the model is with respect to different face expression categories.\n",
        "  - sub-group 1: neutral\n",
        "  - sub-group 2: slightlyhappy\n",
        "  - sub-group 3: happy\n",
        "  - sub-group 4: other\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRpi-2MFyfFT"
      },
      "source": [
        "---\n",
        "# Computing the Age, Gender and Ethnicity Biases on the Validation set\n",
        "- Next, we compute the different bias scores **using the model we obtained after the 2nd stage of training**.\n",
        "- We re-scale the predictions and labels back to the rage of \"ages\" using the normalization factor defined before, to make the analysis easier.\n",
        "- First, we need to **download our \"bias library\"**, which contains the functions used to evaluate the different bias scores:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading our \"bias library\", which contains the functions used to evaluate\n",
        "# the different bias scores\n",
        "!wget http://data.chalearnlap.cvc.uab.cat/Colab_2021/bias_functions.py"
      ],
      "metadata": {
        "id": "Gp0OApElX5Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K2FDPqbyj96"
      },
      "outputs": [],
      "source": [
        "# importing the functions used to evaluate the different biases\n",
        "from bias_functions import age_bias, gender_bias, ethnicity_bias, face_expression_bias\n",
        "\n",
        "# evaluating the different bias scores\n",
        "if(ENABLE_EVALUATION_ST2==True):\n",
        "  # computing the age bias (model_stage_2)\n",
        "  age_bias(predictions_st2_valid_final,Y_valid*100)\n",
        "\n",
        "  # computing the gender bias (model_stage_2)\n",
        "  gender_bias(predictions_st2_valid_final,Y_valid*100,M_valid)\n",
        "\n",
        "  # computing the ethnicity bias (model_stage_2)\n",
        "  ethnicity_bias(predictions_st2_valid_final,Y_valid*100,M_valid)\n",
        "\n",
        "  # computing the face bias (model_stage_2)\n",
        "  face_expression_bias(predictions_st2_valid_final,Y_valid*100,M_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lqbRPFK62iL"
      },
      "source": [
        "---\n",
        "---\n",
        "########### **Bias Mitigation (strategy 1)** ##############\n",
        "---\n",
        "---\n",
        "---\n",
        "# Strategies to improve Accuracy (i.e., to reduce the Error) and mitigate bias: \n",
        "- We will comment about two different strategies: 1) data augmentation and 2) custom loss with weighted samples.\n",
        "\n",
        "# **1) Data augmentation**\n",
        "- Next, we will augment the train set of people having age >= 60, as this group got the highest MAE compared to the other groups (regarding the age attribute only). \n",
        "- Different data augmentation strategies can be used. In this example, we  consider horizontal flip, changing the brighness, gaussian blur and translation.\n",
        "- **Ideally, you should try to mitigate bias with respect to ALL categories and sub-groups**. \n",
        "- **Exploit your creativity as much as you can and avoid simply reusing or making minor changes of this starting-kit.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM8PQrOcCB7n"
      },
      "outputs": [],
      "source": [
        "# loading the train data again (original face images, before preprocessing):\n",
        "X_train = np.load('./data/data_train.npy')\n",
        "\n",
        "# Randomly selecting a random image from train set\n",
        "x = X_train[random.randint(0, len(X_train))]\n",
        "\n",
        "# flip horizontaly\n",
        "x_flipped = cv2.flip(x, 1)\n",
        "\n",
        "# change brightness\n",
        "x_brigth = cv2.cvtColor(x,cv2.COLOR_RGB2HSV)\n",
        "x_brigth[:,:,2] = x_brigth[:,:,2]*.5+np.random.uniform()\n",
        "x_brigth = cv2.cvtColor(x_brigth,cv2.COLOR_HSV2RGB)\n",
        "\n",
        "# gaussian blur (here you can also play with the function parameters)\n",
        "x_blur = cv2.GaussianBlur(x,(5,5),1.0)\n",
        "\n",
        "# translation (randomly translation from -25 to +25 in x and y)\n",
        "rows, cols ,c= x.shape\n",
        "M = np.float32([[1, 0, random.randint(-25, 25)], [0, 1, random.randint(-25, 25)]])\n",
        "x_translate = cv2.warpAffine(x, M, (cols, rows))\n",
        "\n",
        "# Visualizing the augmented data\n",
        "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(15, 15))\n",
        "ax1.imshow(cv2.cvtColor(x, cv2.COLOR_BGR2RGB)) # original image\n",
        "ax2.imshow(cv2.cvtColor(x_flipped, cv2.COLOR_BGR2RGB)) # flip horizontaly\n",
        "ax3.imshow(cv2.cvtColor(x_brigth, cv2.COLOR_BGR2RGB)) # change brightness\n",
        "ax4.imshow(cv2.cvtColor(x_blur, cv2.COLOR_BGR2RGB)) # gaussian blur\n",
        "ax5.imshow(cv2.cvtColor(x_translate, cv2.COLOR_BGR2RGB)) # translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY2uBi2L-ro-"
      },
      "source": [
        "# Applying the data augmentation described above\n",
        "- Note, for each augmented image, we replicate its original label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMcDuU4xFvNv"
      },
      "outputs": [],
      "source": [
        "# aux variables\n",
        "X_train_augmented = []\n",
        "Y_train_augmented = []\n",
        "\n",
        "# loading the train data and labels\n",
        "X_train = np.load('./data/data_train.npy')\n",
        "Y_train = np.load('./data/labels_train.npy')\n",
        "Y_train = Y_train/100\n",
        "print('Train set before augmentation = ', np.array(X_train).shape)\n",
        "\n",
        "# augmenting the data\n",
        "for i in range(0,len(X_train)):\n",
        "  # check if image is in the group 'age >= 60'\n",
        "\n",
        "  if Y_train[i]*100>=60: # here labels are multiplied by 100 as they were normalized to be between [0,1]\n",
        "    # flip\n",
        "    X_train_augmented.append(cv2.flip(X_train[i], 1))\n",
        "    Y_train_augmented.append(Y_train[i]) \n",
        "\n",
        "    # changing brightness\n",
        "    x_aux = cv2.cvtColor(X_train[i],cv2.COLOR_RGB2HSV)\n",
        "    x_aux[:,:,2] = x_aux[:,:,2]*.5+np.random.uniform()\n",
        "    X_train_augmented.append(cv2.cvtColor(x_aux,cv2.COLOR_HSV2RGB))\n",
        "    Y_train_augmented.append(Y_train[i]) \n",
        "\n",
        "    # blur\n",
        "    X_train_augmented.append(cv2.GaussianBlur(X_train[i],(5,5),1.0))\n",
        "    Y_train_augmented.append(Y_train[i])\n",
        "   \n",
        "    # translation\n",
        "    rows, cols ,c= X_train[i].shape\n",
        "    M = np.float32([[1, 0, random.randint(-25, 25)], [0, 1, random.randint(-25, 25)]])\n",
        "    X_train_augmented.append(cv2.warpAffine(X_train[i], M, (cols, rows)))\n",
        "    Y_train_augmented.append(Y_train[i])\n",
        "\n",
        "# adding the augmented images to the train set\n",
        "X_train = np.concatenate((X_train, X_train_augmented))\n",
        "Y_train = np.concatenate((Y_train, Y_train_augmented))\n",
        "print('Train set after augmentation = ', np.array(X_train).shape)\n",
        "\n",
        "\n",
        "# post-processing the train data with respect to ResNet-50 Inputs.\n",
        "for i in range(0,X_train.shape[0]):\n",
        "  x = X_train[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_train[i,] = tf.keras.applications.resnet50.preprocess_input(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiTXleG_Ik4r"
      },
      "source": [
        "---\n",
        "# **Training the 2nd stage again**, from the model obtained at the 1st stage, but now **using the augmented data**\n",
        "- As before, we provide a model already trained (using the augmented data), which will be loaded as default if the code below is not changed. \n",
        "- You can change the boolean variable LOAD_BEST_MODEL_ST2_AUGMENTATION from True to False in order to train the model again, given your redefined strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJdYBDYiJp-q"
      },
      "outputs": [],
      "source": [
        "#--------------------------\n",
        "LOAD_BEST_MODEL_ST2_AUGMENTATION = True\n",
        "#--------------------------\n",
        "\n",
        "if(LOAD_BEST_MODEL_ST2_AUGMENTATION==True):\n",
        "  # downloading the trained model\n",
        "  !wget http://data.chalearnlap.cvc.uab.cat/Colab_2021/best_model_st2_aug.zip\n",
        "  # decompressing the data\n",
        "  with ZipFile('best_model_st2_aug.zip','r') as zip:\n",
        "    zip.extractall()\n",
        "    print('Model decompressed successfully')\n",
        "  # removing the .zip file after extraction  to clean space\n",
        "  !rm best_model_st2_aug.zip\n",
        "\n",
        "else:\n",
        "  # loading the saved model (best model learned at stage 1)\n",
        "  saved_model = tf.keras.models.load_model('/content/gdrive/MyDrive/temp/best_model.h5')\n",
        "  \n",
        "  # setting all layers to traineble\n",
        "  saved_model.trainable = True\n",
        "\n",
        "  # training all layers (2nd stage), given the model saved on stage 1\n",
        "  saved_model.compile(tf.keras.optimizers.Adam(learning_rate=1e-5),loss=tf.keras.losses.MeanSquaredError(),metrics=['mae'])\n",
        "\n",
        "  # defining the early stop criteria\n",
        "  es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "  \n",
        "  # saving the best model (2nd stage) based on val_loss with a different name\n",
        "  mc = tf.keras.callbacks.ModelCheckpoint('/content/gdrive/MyDrive/temp/best_model_2nd_stage_augmentation.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "  history = saved_model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), batch_size=16, epochs=12, shuffle=True, verbose=1, callbacks=[es,mc])\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UY-ci6Yc7VR"
      },
      "source": [
        "# Train History of the provided pre-trained model\n",
        "\n",
        "Epoch 1/12\n",
        "292/292 [==============================] - 63s 194ms/step - loss: 0.2077 - mae: 0.1340 - val_loss: 0.2021 - val_mae: 0.1209\n",
        "Epoch 2/12\n",
        "292/292 [==============================] - 58s 197ms/step - loss: 0.1938 - mae: 0.0973 - val_loss: 0.1987 - val_mae: 0.1116\n",
        "Epoch 3/12\n",
        "292/292 [==============================] - 58s 197ms/step - loss: 0.1884 - mae: 0.0796 - val_loss: 0.1975 - val_mae: 0.1081\n",
        "Epoch 4/12\n",
        "292/292 [==============================] - 58s 198ms/step - loss: 0.1853 - mae: 0.0692 - val_loss: 0.1967 - val_mae: 0.1081\n",
        "Epoch 5/12\n",
        "292/292 [==============================] - 58s 197ms/step - loss: 0.1830 - mae: 0.0617 - val_loss: 0.1946 - val_mae: 0.1035\n",
        "Epoch 6/12\n",
        "292/292 [==============================] - 58s 199ms/step - loss: 0.1819 - mae: 0.0581 - val_loss: 0.1933 - val_mae: 0.1010\n",
        "Epoch 7/12\n",
        "292/292 [==============================] - 58s 198ms/step - loss: 0.1801 - mae: 0.0529 - val_loss: 0.1912 - val_mae: 0.0979\n",
        "Epoch 8/12\n",
        "292/292 [==============================] - 59s 202ms/step - loss: 0.1790 - mae: 0.0506 - val_loss: 0.1898 - val_mae: 0.0956\n",
        "Epoch 9/12\n",
        "292/292 [==============================] - 59s 200ms/step - loss: 0.1776 - mae: 0.0475 - val_loss: 0.1888 - val_mae: 0.0954\n",
        "Epoch 10/12\n",
        "292/292 [==============================] - 57s 196ms/step - loss: 0.1763 - mae: 0.0453 - val_loss: 0.1871 - val_mae: 0.0937\n",
        "Epoch 11/12\n",
        "292/292 [==============================] - 58s 199ms/step - loss: 0.1748 - mae: 0.0424 - val_loss: 0.1858 - val_mae: 0.0925\n",
        "Epoch 12/12\n",
        "292/292 [==============================] - 58s 199ms/step - loss: 0.1736 - mae: 0.0420 - val_loss: 0.1837 - val_mae: 0.0896"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDrT0WwDAMnb"
      },
      "source": [
        "# Evaluating the model (with data augmentation) on the Validation set\n",
        "- Here, you can observe a decrease in Mean Absolute Error (MAE) compared to the predictions obtained by the previously trained models:\n",
        "  - Model trained at the 1st stage: MAE = 16.98603930\n",
        "  - Model trained at the 2nd stage (without data augmentation): MAE = 9.84937248\n",
        "  - Model trained at the 2nd stage (with data augmentation): MAE = 9.05623911"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_8LL1RJAQCp"
      },
      "outputs": [],
      "source": [
        "# loading the saved model\n",
        "if(LOAD_BEST_MODEL_ST2_AUGMENTATION==True):\n",
        "  saved_model_2nd_augm = tf.keras.models.load_model('best_model_2nd_stage_augmentation.h5')\n",
        "else:\n",
        "  saved_model_2nd_augm = tf.keras.models.load_model('/content/gdrive/MyDrive/temp/best_model_2nd_stage_augmentation.h5')\n",
        "\n",
        "\n",
        "#--------------------------\n",
        "ENABLE_EVALUATION_ST2_AUGMENTED = True\n",
        "#--------------------------\n",
        "\n",
        "if(ENABLE_EVALUATION_ST2_AUGMENTED==True):\n",
        "  # predict on the test data\n",
        "  predictions_st2_augmented = saved_model_2nd_augm.predict(X_valid, batch_size=32, verbose=1)\n",
        "  # re-scaling the output predictions (from [0,1] to age range) using the\n",
        "  # the normalization factor mentioned before\n",
        "  predictions_st2_augmented_final = predictions_st2_augmented*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1HljLcyBqI1"
      },
      "outputs": [],
      "source": [
        "if(ENABLE_EVALUATION_ST2_AUGMENTED==True):\n",
        "  # evaluating on validation data\n",
        "  error = []\n",
        "  for i in range(0,len(Y_valid)):\n",
        "    error.append(abs(np.subtract(predictions_st2_augmented_final[i][0],Y_valid[i]*100)))\n",
        "\n",
        "  print('MAE (validation) = %.8f' %(np.mean(error)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWowpmtwB8px"
      },
      "source": [
        "# Computing the Age Bias (augmented data) on the Validation set\n",
        "- Next, if we compare the age bias obtained by the two models (without/with) data augmentation, we can observed the model with augmented data was able to recude the age bias (from 6.40 to 4.65) on the Validation set. In particular, the MAE for group 4 reduced from 19.45 to 16.28, suggesting that our data augmentation had a positive impact in training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F9LfW1ZCBrI"
      },
      "outputs": [],
      "source": [
        "if(ENABLE_EVALUATION_ST2==True):\n",
        "  # computing the age bias (model_stage_2) - previous model without data augmentation\n",
        "  age_bias(predictions_st2_valid_final,Y_valid*100)\n",
        "\n",
        "if(ENABLE_EVALUATION_ST2_AUGMENTED==True):  \n",
        "  # computing the age bias (model_stage_2) - with data augmentation\n",
        "  age_bias(predictions_st2_augmented_final,Y_valid*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjkJNXGilNfo"
      },
      "source": [
        "---\n",
        "---\n",
        "########### **Bias Mitigation (strategy 2)** ##############\n",
        "---\n",
        "---\n",
        "---\n",
        "# Strategies to improve Accuracy (i.e., to reduce the Error):\n",
        "# **2) Custom Loss:** sample weights to deal with inbalanced categories\n",
        "- Next, we will created a \"customized loss\", which gives more weight to people having less samples in train data. For this simple example, **we will consider the age range only**. This way, we believe the model will be able to generalize a little bit better to those particular groups. Note that you are expect to build a more strong strategy, aiming to mitigate the bias with respect to all categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGjU8Xetxl3W"
      },
      "source": [
        "# Load the Train data again (to remove the augmented data) and generate the weigths \n",
        "- First, we will generate a weight for each age group (for g =1 to 4);\n",
        "- The formula used to calculate the weight for each group $j$ is:\n",
        "\n",
        "  $w_j=n_{samples} / (n_{classes} * n_{samples,j}),$\n",
        "\n",
        "  Where\n",
        "\n",
        "    - $w_j$ is the weight for each group $j$,\n",
        "    - $n_{samples}$ is the number of samples in the train set,\n",
        "    - $n_{classes}$ is the number of classes (4 in our case, as we divided the ages in 4 groups),\n",
        "    - $n_{samples,j}$ is the number of samples of class (group) $j$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svMMpvU9xcJR"
      },
      "outputs": [],
      "source": [
        "# loading the train data again (original face images, before preprocessing):\n",
        "X_train = np.load('./data/data_train.npy')\n",
        "Y_train = np.load('./data/labels_train.npy')\n",
        "Y_train = Y_train/100 # normalizing the age values to be between [0,1]\n",
        "\n",
        "# preprocessing the train data with respect to ResNet-50 Inputs.\n",
        "for i in range(0,X_train.shape[0]):\n",
        "  x = X_train[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_train[i,] = tf.keras.applications.resnet50.preprocess_input(x)\n",
        "\n",
        "# counting the number of samples per group in the train data (age attribute only)\n",
        "g1 = g2 = g3 = g4 = 0\n",
        "for i in range(0,Y_train.shape[0]):\n",
        "    if(Y_train[i]*100<20):\n",
        "      g1 +=1\n",
        "    if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n",
        "      g2 +=1\n",
        "    if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n",
        "      g3 +=1\n",
        "    if(Y_train[i]*100>=60):\n",
        "      g4 +=1\n",
        "print('group(s) size = ', [g1, g2, g3, g4])\n",
        "\n",
        "# generating the weights for each group using the equation defined above\n",
        "w = sum(np.array([g1, g2, g3, g4]))/(4*np.array([g1, g2, g3, g4]))\n",
        "print('weights per group = ', w)\n",
        "\n",
        "# creating a vector with same size as Y_train, that will link a particular label to its weight\n",
        "sample_weights = []\n",
        "for i in range(0,Y_train.shape[0]):\n",
        "    if(Y_train[i]*100<20):\n",
        "      sample_weights.append(w[0])\n",
        "    if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n",
        "      sample_weights.append(w[1])\n",
        "    if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n",
        "      sample_weights.append(w[2])\n",
        "    if(Y_train[i]*100>=60):\n",
        "      sample_weights.append(w[3])\n",
        "sample_weights = np.array(sample_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gTtMXGmx5jA"
      },
      "source": [
        "# I) Using the SAMPLE WEIGHTS to train our model and,\n",
        "- Next, you will see the code we used to train our model (2nd stage) from the model we obtained at the 1st stage, using the customized loss option with sample weights.\n",
        "- As default, the code will load the model already trained. \n",
        "- You can change the boolean variable 'LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS' to False to train your model.\n",
        "- Note, now we include other variables ('RESUME_TRAINING' and 'RESUME_FROM_EPOCH') to allow us resuming training, as well as to inform from what epoch we want to resume the trainind, detailed below.\n",
        "\n",
        "# II) illustrating how to train + save + stop training + RESUME TRAINING\n",
        "- **Imagine** you set 'LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS = False', 'NUM_EPOCHS = 12' and 'RESUME_TRAINING = False' to train your model the first time.\n",
        "- Due to Colab limitations, your process stoped the training at the middle of epoch 10, and you saved the best model based on validation loss on epoch 9.\n",
        "- In the above example, you can resume training from epoch 9 by setting the following parameters:\n",
        "  - 'RESUME_TRAINING = True'\n",
        "  - 'RESUME_FROM_EPOCH = 9'\n",
        "\n",
        "- IMPORTANT: to resume training, you will need to monitor the epoch number where your model stopped before resuming the training, and change the defined variables properly. \n",
        "  - Note that the fit function is adapted to receive the sample weights ('sample_weight=sample_weights').\n",
        "  - Also note that the fit function changes if you are training from epoch 0 (initial_epoch=0) or resume training (initial_epoch=RESUME_FROM_EPOCH). \n",
        "  - Finally, note that when you are resuming training, you load your 'best_model_2nd_stage_weighted.h5' instead of the model trained at stage 1 ('best_model.h5').\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjEx3B3lx8Mu"
      },
      "outputs": [],
      "source": [
        "#--------------------------\n",
        "LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS = True\n",
        "NUM_EPOCHS = 12\n",
        "#--------------------------\n",
        "RESUME_TRAINING = False\n",
        "RESUME_FROM_EPOCH = 9\n",
        "#--------------------------\n",
        "\n",
        "if(LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS==True):\n",
        "  # downloading the trained model\n",
        "  !wget https://data.chalearnlap.cvc.uab.cat/Colab_2021/best_model_weighted.zip\n",
        "  # decompressing the data\n",
        "  with ZipFile('best_model_weighted.zip','r') as zip:\n",
        "    zip.extractall()\n",
        "    print('Model decompressed successfully')\n",
        "  # removing the .zip file after extraction  to clean space\n",
        "  !rm best_model_weighted.zip\n",
        "\n",
        "else:\n",
        "  # loading the saved model (best model learned at stage 1)\n",
        "  if(RESUME_TRAINING == False):\n",
        "    saved_model = tf.keras.models.load_model('/content/gdrive/MyDrive/temp/best_model.h5') # load model from stage 1\n",
        "  else:\n",
        "    # resume training (stage 2)\n",
        "    saved_model = tf.keras.models.load_model('/content/gdrive/MyDrive/temp/best_model_2nd_stage_weighted.h5')\n",
        "  \n",
        "  # setting all layers to traineble\n",
        "  saved_model.trainable = True\n",
        "\n",
        "  #=================================================\n",
        "  # training all layers (2nd stage), given the model saved on stage 1\n",
        "  saved_model.compile(tf.keras.optimizers.Adam(learning_rate=1e-5),loss=tf.keras.losses.MeanSquaredError(),metrics=['mae'])\n",
        "  #=================================================\n",
        "\n",
        "  # defining the early stop criteria\n",
        "  es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "  mc = tf.keras.callbacks.ModelCheckpoint('/content/gdrive/MyDrive/temp/best_model_2nd_stage_weighted.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "  if(RESUME_TRAINING == False):  \n",
        "    history = saved_model.fit(X_train, Y_train, sample_weight=sample_weights, validation_data=(X_valid, Y_valid), batch_size=16, epochs=NUM_EPOCHS, initial_epoch=0, shuffle=True, verbose=1, callbacks=[es,mc])\n",
        "  else:\n",
        "    history = saved_model.fit(X_train, Y_train, sample_weight=sample_weights, validation_data=(X_valid, Y_valid), batch_size=16, epochs=NUM_EPOCHS, initial_epoch=RESUME_FROM_EPOCH, shuffle=True, verbose=1, callbacks=[es,mc])\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7kqieNu7zDS"
      },
      "source": [
        "# Making predictions on the Validation set and Evaluating\n",
        "- Note that in this case, the model obtained MAE = 11.32504168, which is not the best score compared to the ones obtained before. However, are the evaluated biases better? Let's see!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYq1tCet70bN"
      },
      "outputs": [],
      "source": [
        "if(LOAD_BEST_MODEL_ST2_WEIGHTED_LOSS==True):\n",
        "  saved_model_2nd_weighted = tf.keras.models.load_model('best_model_2nd_stage_weighted.h5')\n",
        "else:\n",
        "  saved_model_2nd_weighted = tf.keras.models.load_model('/content/gdrive/MyDrive/temp/best_model_2nd_stage_weighted.h5')\n",
        "\n",
        "#--------------------------\n",
        "ENABLE_EVALUATION_WEIGHTED = True\n",
        "#--------------------------\n",
        "\n",
        "if(ENABLE_EVALUATION_WEIGHTED==True):\n",
        "  # predict on the test data\n",
        "  predictions_st2_weighted = saved_model_2nd_weighted.predict(X_valid, batch_size=32, verbose=1)\n",
        "  # re-scaling the output predictions (from [0,1] to age range) using the\n",
        "  # the normalization factor mentioned before\n",
        "  predictions_st2_weighted_final = predictions_st2_weighted*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCyMYVPgILxq"
      },
      "outputs": [],
      "source": [
        "if(ENABLE_EVALUATION_WEIGHTED==True):\n",
        "  # evaluating on Validation data\n",
        "  error = []\n",
        "  for i in range(0,len(Y_valid)):\n",
        "    error.append(abs(np.subtract(predictions_st2_weighted_final[i][0],Y_valid[i]*100)))\n",
        "\n",
        "  print('MAE (Validation) = %.8f' %(np.mean(error)))\n",
        "\n",
        "# printing some predictions\n",
        "for i in range(0,20):\n",
        "  print('predicted age = %.3f - Ground truth = %.3f' %(predictions_st2_weighted_final[i], Y_valid[i]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpJCmvGQXDIS"
      },
      "source": [
        "## Comparing the 2nd stage of training on validation set: \n",
        "**case a)** without augmentation/custom loss *vs.* **case b)** without augmentation but with custom loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjrDG9AHLoPf"
      },
      "outputs": [],
      "source": [
        "if(ENABLE_EVALUATION_ST2==True and ENABLE_EVALUATION_WEIGHTED==True):\n",
        "  age_bias(predictions_st2_valid_final,Y_valid*100)\n",
        "  age_bias(predictions_st2_weighted_final,Y_valid*100)\n",
        "\n",
        "  gender_bias(predictions_st2_valid_final,Y_valid*100,M_valid)\n",
        "  gender_bias(predictions_st2_weighted_final,Y_valid*100,M_valid)\n",
        "\n",
        "  ethnicity_bias(predictions_st2_valid_final,Y_valid*100,M_valid)\n",
        "  ethnicity_bias(predictions_st2_weighted_final,Y_valid*100,M_valid)\n",
        "\n",
        "  face_expression_bias(predictions_st2_valid_final,Y_valid*100,M_valid)\n",
        "  face_expression_bias(predictions_st2_weighted_final,Y_valid*100,M_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**case a)** without augmentation/custom loss *vs.* **case b)** without augmentation but with custom loss.\n",
        "- Age bias:\n",
        "  - case a: 6.408208847045898\n",
        "  - case b: 2.877239227294922\n",
        "- Gender bias:\n",
        "  - case a: 0.49919033\n",
        "  - case b: 0.60023785\n",
        "- Ethnicity bias:\n",
        "  - case a: 1.4386415481567383\n",
        "  - case b: 1.1498057047526042\n",
        "- Face Expression bias:\n",
        "  - case a: 0.4711581865946452\n",
        "  - case b: 0.7978432973225912\n",
        "\n",
        "As it can be observed, the model with custom loss and weighted samples obtained smaller bias scores on Age and Ethnicity attributes (on the validation set), even if the weigts were defined based on age attribute only. The strongest effect was on Age attribute, with a reduction from 6.4 to 2.8. The evaluated bias on other attributes increased a bit (i.e., gender and face expression), but still keeping below 1, which can be considered acceptable.  "
      ],
      "metadata": {
        "id": "WQXGcN771qM8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-sTrrsB4oR6"
      },
      "source": [
        "---\n",
        "---\n",
        "# Practical Exercises \n",
        "We defined a serie of practical exercises where you will be able to:\n",
        "- Play with data augmentation;\n",
        "- Play with custom loss (without data augmentation)\n",
        "- Be free to propose any strategy (optional excercise)\n",
        "- Compare the different methods and results incrementaly.\n",
        "\n",
        "**Additional details and deliverable deadlines can be found on the Virtual Campus.**\n",
        "\n",
        "- We will use **Codalab** to motivate the students, as they can submit their results on the platform, compete with each other and improve their solutions, but **the ranking shown in the leaderboard will not be considered in the evaluation. This is to justify that more creative solutions will be priefered even if they don't provide the best results.**\n",
        "- You will be requested to share with the lectors your final **Colab file** (with a clean code and well documented) and a **Report document** where you describe your solution, defined experiments and results, with a clear and progressive analysis). Please, check the class material associated with the practical sessions on the Virtual Campus for more details. \n",
        "\n",
        "---\n",
        " \n",
        " ## Saving your predictions **on the Test set** as a file.zip \n",
        " \n",
        "Next, you can find a piece of code used to save the predicted results as a zip file that can be uploaded to our compatition. You can adapt it based on your needs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- saving predictions task 1 (test set) --\n",
        "# data augmentation /task 1\n",
        "saved_model_2nd_augm = tf.keras.models.load_model('best_model_2nd_stage_augmentation.h5')\n",
        "predictions_st2_augmented_test = saved_model_2nd_augm.predict(X_test, batch_size=32, verbose=1)\n",
        "predictions_st2_augmented_test_f = predictions_st2_augmented_test*100\n",
        "# saving the predictions as a csv file\n",
        "import csv\n",
        "with open('predictions.csv', 'w') as csvFile:\n",
        "  writer = csv.writer(csvFile)\n",
        "  writer.writerows(predictions_st2_augmented_test_f)\n",
        "csvFile.close()\n",
        "# compressing the csv file (to be submitted to codalab as prediction)\n",
        "! zip baseline_results_task_1.zip predictions.csv\n",
        "# -- saving predictions task 1 (test set) --"
      ],
      "metadata": {
        "id": "Uc5aDJ1WpkZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- saving predictions task 2 (test set) --\n",
        "# custom loss /task 2\n",
        "saved_model_2nd_weighted = tf.keras.models.load_model('best_model_2nd_stage_weighted.h5')\n",
        "predictions_st2_custom_loss_test = saved_model_2nd_weighted.predict(X_test, batch_size=32, verbose=1)\n",
        "predictions_st2_custom_loss_test_f = predictions_st2_custom_loss_test*100\n",
        "# saving the predictions as a csv file\n",
        "import csv\n",
        "with open('predictions.csv', 'w') as csvFile:\n",
        "  writer = csv.writer(csvFile)\n",
        "  writer.writerows(predictions_st2_custom_loss_test_f)\n",
        "csvFile.close()\n",
        "# compressing the csv file (to be submitted to codalab as prediction)\n",
        "! zip baseline_results_task_2.zip predictions.csv\n",
        "# -- saving predictions task 1 (test set) --"
      ],
      "metadata": {
        "id": "hRuk9RDOqb2w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}